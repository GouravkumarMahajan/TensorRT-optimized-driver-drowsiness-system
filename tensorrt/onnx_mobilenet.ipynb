{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 22:16:19.813167: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-03 22:16:19.813224: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-03 22:16:19.813256: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-03 22:16:19.821213: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorrt as trt\n",
    "from onnx import ModelProto\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from tensorflow.keras.applications import MobileNet, VGG16\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from mtcnn import MTCNN\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import tf2onnx\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import onnx_graphsurgeon as gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOBILENET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conversion of tensorflow to onnx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 22:15:44.367273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:44.372757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:44.372968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:44.374544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:44.374718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:44.374867: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:44.423769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:44.424003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:44.424173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:44.424322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1087 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-09-23 22:15:45.380081: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.380261: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2024-09-23 22:15:45.380368: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-09-23 22:15:45.380679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.380846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.380994: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.381193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.381350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.381506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1087 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-09-23 22:15:45.929238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.929531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.929689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.929920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.930085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.930219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1087 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-09-23 22:15:45.995788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.996010: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2024-09-23 22:15:45.996179: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-09-23 22:15:45.996471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.996658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.996808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.997011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.997166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 22:15:45.997297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1087 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Load your TensorFlow model\n",
    "model = tf.keras.models.load_model('/home/gourav/Desktop/vss/capstone/best_model.h5')\n",
    "\n",
    "# Specify the input shape with dynamic batch size\n",
    "input_signature = [tf.TensorSpec([1, 224, 224, 3], tf.float32, name=\"input\")]\n",
    "\n",
    "# Convert the model to ONNX\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=input_signature)\n",
    "\n",
    "# Save the ONNX model\n",
    "with open(\"model_mobilenet.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knowing tensorflow model throughput and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1269 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 22:06:11.463500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1269/1269 [==============================] - 9s 5ms/step - loss: 0.0798 - binary_accuracy: 0.9827 - precision_1: 0.9767 - recall_1: 0.9890 - auc_1: 0.9826\n",
      "TensorFlow Test Loss: 0.07977847754955292\n",
      "TensorFlow Test Accuracy: 0.9826635122299194\n",
      "TensorFlow Test Accuracy (%): 98.26635122299194\n",
      "TensorFlow Throughput (samples/second): 141.78509883080616\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the TensorFlow model\n",
    "model = tf.keras.models.load_model('/home/gourav/Desktop/vss/capstone/best_model.h5')\n",
    "\n",
    "# Set up the test data generator\n",
    "test_data_generator = ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_generator = test_data_generator.flow_from_directory(\n",
    "    '/home/gourav/Desktop/vss/capstone/dataset/test',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=1,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate the TensorFlow model\n",
    "start_time = time.time()\n",
    "tf_result = model.evaluate(test_generator)\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate throughput\n",
    "total_samples = len(test_generator.filenames)\n",
    "total_time = end_time - start_time\n",
    "throughput = total_samples / total_time\n",
    "\n",
    "# Print the results\n",
    "print(\"TensorFlow Test Loss:\", tf_result[0])\n",
    "print(\"TensorFlow Test Accuracy:\", tf_result[1])\n",
    "print(\"TensorFlow Test Accuracy (%):\", tf_result[1]*100)\n",
    "print(\"TensorFlow Throughput (samples/second):\", throughput)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model accuracy and throughput for onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1269 images belonging to 2 classes.\n",
      "ONNX Test Accuracy: 0.9826635145784082\n",
      "ONNX Test Accuracy (%): 98.26635145784081\n",
      "ONNX Throughput (samples/second): 218.0456164532443\n",
      "ONNX Total inference time (seconds): 5.819883108139038\n"
     ]
    }
   ],
   "source": [
    "# Load the ONNX model\n",
    "model_path = './model_mobilenet.onnx'\n",
    "session = onnxruntime.InferenceSession(model_path)\n",
    "\n",
    "# Get model input details\n",
    "input_name = session.get_inputs()[0].name\n",
    "input_shape = session.get_inputs()[0].shape\n",
    "_, height, width, channels = input_shape\n",
    "\n",
    "# Set up the test data generator\n",
    "test_data_generator = ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_generator = test_data_generator.flow_from_directory(\n",
    "    '/home/gourav/Desktop/vss/capstone/dataset/test',\n",
    "    target_size=(height, width),\n",
    "    batch_size=1,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate the ONNX model\n",
    "correct_predictions = 0\n",
    "total_samples = len(test_generator.filenames)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(total_samples):\n",
    "    image, label = test_generator.next()\n",
    "    \n",
    "    # Process the single sample\n",
    "    output = session.run(None, {input_name: image})[0]\n",
    "    predicted_class = (output > 0.5).astype(int).flatten()\n",
    "    correct_predictions += int(predicted_class == label)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = correct_predictions / total_samples\n",
    "total_time = end_time - start_time\n",
    "throughput = total_samples / total_time\n",
    "\n",
    "# Print the results\n",
    "print(\"ONNX Test Accuracy:\", accuracy)\n",
    "print(\"ONNX Test Accuracy (%):\", accuracy * 100)\n",
    "print(\"ONNX Throughput (samples/second):\", throughput)\n",
    "print(\"ONNX Total inference time (seconds):\", total_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## printing shapes of onnx and tensorfloow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorFlow model\n",
    "model = tf.keras.models.load_model('/home/gourav/Desktop/vss/capstone/best_model.h5')\n",
    "\n",
    "# Print input and output shapes and types\n",
    "print(\"TensorFlow Model Inputs:\")\n",
    "for layer in model.inputs:\n",
    "    print(f\"Name: {layer.name}, Shape: {layer.shape}, Type: {layer.dtype}\")\n",
    "\n",
    "print(\"\\nTensorFlow Model Outputs:\")\n",
    "for layer in model.outputs:\n",
    "    print(f\"Name: {layer.name}, Shape: {layer.shape}, Type: {layer.dtype}\")\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = '/home/gourav/Desktop/vss/tensorrt/model.onnx'\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Print input and output shapes and types\n",
    "print(\"ONNX Model Inputs:\")\n",
    "for input in onnx_model.graph.input:\n",
    "    input_type = input.type.tensor_type\n",
    "    shape = [dim.dim_value for dim in input_type.shape.dim]\n",
    "    print(f\"Name: {input.name}, Shape: {shape}, Type: {input_type.elem_type}\")\n",
    "\n",
    "print(\"\\nONNX Model Outputs:\")\n",
    "for output in onnx_model.graph.output:\n",
    "    output_type = output.type.tensor_type\n",
    "    shape = [dim.dim_value for dim in output_type.shape.dim]\n",
    "    print(f\"Name: {output.name}, Shape: {shape}, Type: {output_type.elem_type}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting to TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/23/2024-22:09:55] [TRT] [W] onnx2trt_utils.cpp:377: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "def build_engine(onnx_path, shape=[1, 224, 224, 3]):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network(1) as network, builder.create_builder_config() as config, trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 256 << 20)  # 256 MiB\n",
    "        \n",
    "        # Parse ONNX model\n",
    "        with open(onnx_path, 'rb') as model:\n",
    "            if not parser.parse(model.read()):\n",
    "                for error in range(parser.num_errors):\n",
    "                    print(parser.get_error(error))\n",
    "                raise RuntimeError(\"Failed to parse ONNX model.\")\n",
    "        \n",
    "        # Set the input shape for the network\n",
    "        network.get_input(0).shape = shape\n",
    "        \n",
    "        # Build the serialized engine\n",
    "        serialized_engine = builder.build_serialized_network(network, config)\n",
    "        if serialized_engine is None:\n",
    "            raise RuntimeError(\"Failed to build TensorRT engine.\")\n",
    "        \n",
    "        # Deserialize the engine\n",
    "        engine = trt.Runtime(TRT_LOGGER).deserialize_cuda_engine(serialized_engine)\n",
    "        return engine\n",
    "\n",
    "def save_engine(engine, file_name):\n",
    "    buf = engine.serialize()\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(buf)\n",
    "\n",
    "def load_engine(trt_runtime, plan_path):\n",
    "    with open(plan_path, 'rb') as f:\n",
    "        engine_data = f.read()\n",
    "    engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "    return engine\n",
    "\n",
    "# Define paths and parameters\n",
    "engine_name = \"ssd_mobilenet_fp32.plan\"\n",
    "onnx_path = \"/home/gourav/Desktop/vss/tensorrt/model_mobilenet.onnx\"\n",
    "batch_size = 1\n",
    "\n",
    "# Load ONNX model to get input shape\n",
    "model = ModelProto()\n",
    "with open(onnx_path, \"rb\") as f:\n",
    "    model.ParseFromString(f.read())\n",
    "\n",
    "# Extract dimensions from ONNX model\n",
    "d0 = model.graph.input[0].type.tensor_type.shape.dim[1].dim_value\n",
    "d1 = model.graph.input[0].type.tensor_type.shape.dim[2].dim_value\n",
    "d2 = model.graph.input[0].type.tensor_type.shape.dim[3].dim_value\n",
    "shape = [batch_size, d0, d1, d2]\n",
    "\n",
    "# Build and save TensorRT engine\n",
    "engine = build_engine(onnx_path, shape=shape)\n",
    "save_engine(engine, engine_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/23/2024-22:10:53] [TRT] [W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[09/23/2024-22:10:53] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[09/23/2024-22:10:53] [TRT] [W] Check verbose logs for the list of affected weights.\n",
      "[09/23/2024-22:10:53] [TRT] [W] - 23 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[09/23/2024-22:10:53] [TRT] [W] - 13 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "from onnx import ModelProto\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "def build_engine(onnx_path, shape=[1, 224, 224, 3], use_fp16=False, use_int8=False, calibration_data=None):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network(1) as network, builder.create_builder_config() as config, trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "        # Set memory pool limit\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 256 << 20)  # 256 MiB\n",
    "        \n",
    "        # Enable FP16 precision\n",
    "        if use_fp16:\n",
    "            config.set_flag(trt.BuilderFlag.FP16)\n",
    "        \n",
    "        # Enable INT8 precision\n",
    "        if use_int8:\n",
    "            if calibration_data is None:\n",
    "                raise ValueError(\"Calibration data must be provided for INT8 precision.\")\n",
    "            config.set_flag(trt.BuilderFlag.INT8)\n",
    "            # Create a calibration profile and set it up\n",
    "            calib = trt.IInt8Calibrator()\n",
    "            config.int8_calibrator = calib\n",
    "            # You need to implement a proper calibration method for your dataset\n",
    "\n",
    "        # Parse ONNX model\n",
    "        with open(onnx_path, 'rb') as model:\n",
    "            if not parser.parse(model.read()):\n",
    "                for error in range(parser.num_errors):\n",
    "                    print(parser.get_error(error))\n",
    "                raise RuntimeError(\"Failed to parse ONNX model.\")\n",
    "        \n",
    "        # Set the input shape for the network\n",
    "        network.get_input(0).shape = shape\n",
    "        \n",
    "        # Build the serialized engine\n",
    "        serialized_engine = builder.build_serialized_network(network, config)\n",
    "        if serialized_engine is None:\n",
    "            raise RuntimeError(\"Failed to build TensorRT engine.\")\n",
    "        \n",
    "        # Deserialize the engine\n",
    "        engine = trt.Runtime(TRT_LOGGER).deserialize_cuda_engine(serialized_engine)\n",
    "        return engine\n",
    "\n",
    "def save_engine(engine, file_name):\n",
    "    buf = engine.serialize()\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(buf)\n",
    "\n",
    "def load_engine(trt_runtime, plan_path):\n",
    "    with open(plan_path, 'rb') as f:\n",
    "        engine_data = f.read()\n",
    "    engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "    return engine\n",
    "\n",
    "# Define paths and parameters\n",
    "engine_name = \"ssd_mobilenet_fp16.plan\"\n",
    "onnx_path = \"/home/gourav/Desktop/vss/tensorrt/model_mobilenet.onnx\"\n",
    "batch_size = 1\n",
    "\n",
    "# Load ONNX model to get input shape\n",
    "model = ModelProto()\n",
    "with open(onnx_path, \"rb\") as f:\n",
    "    model.ParseFromString(f.read())\n",
    "\n",
    "# Extract dimensions from ONNX model\n",
    "d0 = model.graph.input[0].type.tensor_type.shape.dim[1].dim_value\n",
    "d1 = model.graph.input[0].type.tensor_type.shape.dim[2].dim_value\n",
    "d2 = model.graph.input[0].type.tensor_type.shape.dim[3].dim_value\n",
    "shape = [batch_size, d0, d1, d2]\n",
    "\n",
    "# Build and save TensorRT engine with FP16 and INT8 precision\n",
    "use_fp16 = True  # Set to True to enable FP16 optimization\n",
    "use_int8 = False  # Set to True to enable INT8 optimization\n",
    "calibration_data = None  # Provide calibration data for INT8 if enabled\n",
    "\n",
    "engine = build_engine(onnx_path, shape=shape, use_fp16=use_fp16, use_int8=use_int8, calibration_data=calibration_data)\n",
    "save_engine(engine, engine_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorRT engine and allocate buffers\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "def load_engine(trt_runtime, plan_path):\n",
    "    with open(plan_path, 'rb') as f:\n",
    "        engine_data = f.read()\n",
    "    engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "    return engine\n",
    "def nptype_fix(trt_type):\n",
    "    import numpy as np\n",
    "    mapping = {\n",
    "        trt.DataType.FLOAT: np.float32,\n",
    "        trt.DataType.HALF: np.float16,\n",
    "        trt.DataType.INT8: np.int8,\n",
    "        trt.DataType.INT32: np.int32,\n",
    "        trt.DataType.BOOL: np.bool_,  # Use np.bool_ instead of np.bool\n",
    "    }\n",
    "    return mapping[trt_type]\n",
    "\n",
    "# Then modify your allocate_buffers function:\n",
    "def allocate_buffers(engine):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    stream = cuda.Stream()\n",
    "    for binding in engine:\n",
    "        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
    "        dtype = nptype_fix(engine.get_binding_dtype(binding))  # Use the new function here\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        bindings.append(int(device_mem))\n",
    "        if engine.binding_is_input(binding):\n",
    "            inputs.append({'host': host_mem, 'device': device_mem})\n",
    "        else:\n",
    "            outputs.append({'host': host_mem, 'device': device_mem})\n",
    "    return inputs, outputs, bindings, stream\n",
    "\n",
    "\n",
    "def do_inference(context, bindings, inputs, outputs, stream):\n",
    "    # Transfer input data to the GPU.\n",
    "    [cuda.memcpy_htod_async(inp['device'], inp['host'], stream) for inp in inputs]\n",
    "    \n",
    "    # Run inference.\n",
    "    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
    "    \n",
    "    # Transfer predictions back from the GPU.\n",
    "    [cuda.memcpy_dtoh_async(out['host'], out['device'], stream) for out in outputs]\n",
    "    \n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    \n",
    "    # Return the output data.\n",
    "    return [out['host'] for out in outputs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/23/2024-22:12:01] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[09/23/2024-22:12:01] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "Found 1269 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5145/615562230.py:27: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
      "/tmp/ipykernel_5145/615562230.py:27: DeprecationWarning: Use network created with NetworkDefinitionCreationFlag::EXPLICIT_BATCH flag instead.\n",
      "  size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
      "/tmp/ipykernel_5145/615562230.py:28: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  dtype = nptype_fix(engine.get_binding_dtype(binding))  # Use the new function here\n",
      "/tmp/ipykernel_5145/615562230.py:32: DeprecationWarning: Use get_tensor_mode instead.\n",
      "  if engine.binding_is_input(binding):\n",
      "/tmp/ipykernel_5145/132289091.py:13: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  input_shape = engine.get_binding_shape(0)  # Assuming input is at index 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT FP32 Test Accuracy: 0.9826635145784082\n",
      "TensorRT FP32 Test Accuracy (%): 98.26635145784081\n",
      "TensorRT FP32 Throughput (samples/second): 492.4043798744254\n",
      "TensorRT FP32 Total inference time (seconds): 2.5771501064300537\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the TensorRT engine\n",
    "engine_path = './ssd_mobilenet_fp32.plan'\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "engine = load_engine(trt_runtime, engine_path)\n",
    "\n",
    "# Create an execution context\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "# Allocate buffers for input and output\n",
    "inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "\n",
    "# Set up the test data generator\n",
    "input_shape = engine.get_binding_shape(0)  # Assuming input is at index 0\n",
    "batch_size, height, width, channels = input_shape\n",
    "test_data_generator = ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_generator = test_data_generator.flow_from_directory(\n",
    "    '/home/gourav/Desktop/vss/capstone/dataset/test',\n",
    "    target_size=(height, width),\n",
    "    batch_size=1,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate the TensorRT model\n",
    "correct_predictions = 0\n",
    "total_samples = len(test_generator.filenames)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(total_samples):\n",
    "    image, label = test_generator.next()\n",
    "    \n",
    "    # Prepare input data\n",
    "    inputs[0]['host'] = np.ascontiguousarray(image.astype(np.float32))\n",
    "    \n",
    "    # Perform inference\n",
    "    output = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)[0]\n",
    "    \n",
    "    # Process the output\n",
    "    predicted_class = (output > 0.5).astype(int).flatten()\n",
    "    correct_predictions += int(predicted_class == label)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = correct_predictions / total_samples\n",
    "total_time = end_time - start_time\n",
    "throughput = total_samples / total_time\n",
    "\n",
    "# Print the results\n",
    "print(\"TensorRT FP32 Test Accuracy:\", accuracy)\n",
    "print(\"TensorRT FP32 Test Accuracy (%):\", accuracy * 100)\n",
    "print(\"TensorRT FP32 Throughput (samples/second):\", throughput)\n",
    "print(\"TensorRT FP32 Total inference time (seconds):\", total_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/23/2024-22:12:30] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[09/23/2024-22:12:30] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "Found 1269 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5145/615562230.py:27: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
      "/tmp/ipykernel_5145/615562230.py:27: DeprecationWarning: Use network created with NetworkDefinitionCreationFlag::EXPLICIT_BATCH flag instead.\n",
      "  size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
      "/tmp/ipykernel_5145/615562230.py:28: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  dtype = nptype_fix(engine.get_binding_dtype(binding))  # Use the new function here\n",
      "/tmp/ipykernel_5145/615562230.py:32: DeprecationWarning: Use get_tensor_mode instead.\n",
      "  if engine.binding_is_input(binding):\n",
      "/tmp/ipykernel_5145/1071714825.py:13: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  input_shape = engine.get_binding_shape(0)  # Assuming input is at index 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT FP16 Test Accuracy: 0.9818754925137904\n",
      "TensorRT FP16 Test Accuracy (%): 98.18754925137904\n",
      "TensorRT FP16 Throughput (samples/second): 683.9236832414038\n",
      "TensorRT FP16 Total inference time (seconds): 1.8554701805114746\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the TensorRT engine\n",
    "engine_path = './ssd_mobilenet_fp16.plan'\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "engine = load_engine(trt_runtime, engine_path)\n",
    "\n",
    "# Create an execution context\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "# Allocate buffers for input and output\n",
    "inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "\n",
    "# Set up the test data generator\n",
    "input_shape = engine.get_binding_shape(0)  # Assuming input is at index 0\n",
    "batch_size, height, width, channels = input_shape\n",
    "test_data_generator = ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_generator = test_data_generator.flow_from_directory(\n",
    "    '/home/gourav/Desktop/vss/capstone/dataset/test',\n",
    "    target_size=(height, width),\n",
    "    batch_size=1,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate the TensorRT model\n",
    "correct_predictions = 0\n",
    "total_samples = len(test_generator.filenames)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(total_samples):\n",
    "    image, label = test_generator.next()\n",
    "    \n",
    "    # Prepare input data\n",
    "    inputs[0]['host'] = np.ascontiguousarray(image.astype(np.float32))\n",
    "    \n",
    "    # Perform inference\n",
    "    output = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)[0]\n",
    "    \n",
    "    # Process the output\n",
    "    predicted_class = (output > 0.5).astype(int).flatten()\n",
    "    correct_predictions += int(predicted_class == label)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = correct_predictions / total_samples\n",
    "total_time = end_time - start_time\n",
    "throughput = total_samples / total_time\n",
    "\n",
    "# Print the results\n",
    "print(\"TensorRT FP16 Test Accuracy:\", accuracy)\n",
    "print(\"TensorRT FP16 Test Accuracy (%):\", accuracy * 100)\n",
    "print(\"TensorRT FP16 Throughput (samples/second):\", throughput)\n",
    "print(\"TensorRT FP16 Total inference time (seconds):\", total_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.5.3.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trt.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
